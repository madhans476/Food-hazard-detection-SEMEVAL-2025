{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "779dc527-c01b-497f-a9fe-c44acdcb48c3",
      "metadata": {
        "id": "779dc527-c01b-497f-a9fe-c44acdcb48c3"
      },
      "outputs": [],
      "source": [
        "! pip install evaluate\n",
        "! pip install --force-reinstall --pre torch --index-url https://download.pytorch.org/whl/nightly/cu117\n",
        "! pip install peft\n",
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "# ! pip install /kaggle/input/llm-peft-pkg/pyarrow_hotfix-0.6-py3-none-any.whl\n",
        "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
        "import evaluate\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "# HuggingFace and WandB login\n",
        "token = 'give_your_hugging_face_token'\n",
        "login(token)\n",
        "os.environ[\"WANDB_API_KEY\"] = \"give_your_WANDB_API_key\"\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"TRAIN-DATASET-PATH\")\n",
        "\n",
        "# Map 'product-category' to numerical labels\n",
        "label2id = {\n",
        "    \"alcoholic beverages\": 0, \"cereals and bakery products\": 1, \"cocoa and cocoa preparations, coffee and tea\": 2,\n",
        "    \"confectionery\": 3, \"dietetic foods, food supplements, fortified foods\": 4, \"fats and oils\": 5,\n",
        "    \"feed materials\": 6, \"food additives and flavourings\": 7, \"food contact materials\": 8, \"fruits and vegetables\": 9,\n",
        "    \"herbs and spices\": 10, \"honey and royal jelly\": 11, \"ices and desserts\": 12, \"meat, egg and dairy products\": 13,\n",
        "    \"non-alcoholic beverages\": 14, \"nuts, nut products and seeds\": 15, \"other food product / mixed\": 16,\n",
        "    \"pet feed\": 17, \"prepared dishes and snacks\": 18, \"seafood\": 19, \"soups, broths, sauces and condiments\": 20,\n",
        "    \"sugars and syrups\": 21\n",
        "}\n",
        "\n",
        "df['label'] = df['product-category'].map(label2id)\n",
        "df['input'] = df[\"title\"]+\": \"+df['text']\n",
        "df = df[['input', 'label']]\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "dataset = DatasetDict({'train': train_dataset, 'validation': val_dataset})\n",
        "\n",
        "# Define model checkpoint and label mappings\n",
        "model_checkpoint = 'openai-community/gpt2-large'\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint, num_labels=22, id2label=id2label, label2id=label2id, device_map = 'auto'\n",
        ")\n",
        "print(model)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "# Add pad token if none exists\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Tokenize function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"input\"],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Metrics\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "    return {\"accuracy\": accuracy_score}\n",
        "\n",
        "# PEFT configuration\n",
        "peft_config = LoraConfig(\n",
        "    task_type=\"SEQ_CLS\",\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\n",
        "        \"c_attn\",\n",
        "        \"c_proj\",\n",
        "        \"c_fc\",\n",
        "        \"mlp.c_proj\",\n",
        "    ]\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "print(model.print_trainable_parameters())\n",
        "\n",
        "# Training arguments\n",
        "lr = 2e-5\n",
        "batch_size = 8\n",
        "num_epochs = 5\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"gpt2-final-product\", # Folder where the checkpoints are stored\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"gpt2-final-product\")\n",
        "tokenizer.save_pretrained(\"gpt2-final-product\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "prediction_data = pd.read_csv(\"TEST DATASET PATH\")\n",
        "prediction_data['input'] = prediction_data['title']+\": \"+prediction_data['text']\n",
        "\n",
        "prediction_dataset = Dataset.from_pandas(prediction_data)\n",
        "\n",
        "# Tokenize the prediction data\n",
        "def tokenize_for_prediction(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"input\"],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "tokenized_prediction_dataset = prediction_dataset.map(tokenize_for_prediction, batched=True)\n",
        "\n",
        "raw_predictions = trainer.predict(tokenized_prediction_dataset)\n",
        "\n",
        "predictions = np.argmax(raw_predictions.predictions, axis=1)\n",
        "\n",
        "# Map numerical labels back to category names\n",
        "predicted_labels = [id2label[pred] for pred in predictions]\n",
        "\n",
        "prediction_data[\"predicted-product-category\"] = predicted_labels\n",
        "# prediction_data.to_csv(\"predictions.csv\", index=False)\n",
        "\n",
        "print(prediction_data)\n"
      ],
      "metadata": {
        "id": "OgMxybObsOPh"
      },
      "id": "OgMxybObsOPh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(prediction_data[\"product-category\"], prediction_data[\"predicted-product-category\"])\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "C8lHio--smsp"
      },
      "id": "C8lHio--smsp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0bMveGaJvL5y"
      },
      "id": "0bMveGaJvL5y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "27ugpAzcvL2S"
      },
      "id": "27ugpAzcvL2S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UVSMHotuvLy6"
      },
      "id": "UVSMHotuvLy6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I3GJAzmRvLvL"
      },
      "id": "I3GJAzmRvLvL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HRsa05DMvLqj"
      },
      "id": "HRsa05DMvLqj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install evaluate\n",
        "! pip install --force-reinstall --pre torch --index-url https://download.pytorch.org/whl/nightly/cu117\n",
        "! pip install peft\n",
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    DataCollatorWithPadding,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "# ! pip install /kaggle/input/llm-peft-pkg/pyarrow_hotfix-0.6-py3-none-any.whl\n",
        "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
        "import evaluate\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "# HuggingFace and WandB login\n",
        "token = 'give_your_hugging_face_token'\n",
        "login(token)\n",
        "os.environ[\"WANDB_API_KEY\"] = \"give_your_WANDB_API_key\"\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"TRAIN-DATASET-PATH\")\n",
        "\n",
        "# Map 'hazard-category' to numerical labels\n",
        "label2id = {\n",
        "    \"allergens\": 0,\n",
        "    \"biological\": 1,\n",
        "    \"chemical\": 2,\n",
        "    \"food additives and flavourings\": 3,\n",
        "    \"foreign bodies\": 4,\n",
        "    \"fraud\": 5,\n",
        "    \"migration\": 6,\n",
        "    \"organoleptic aspects\": 7,\n",
        "    \"other hazard\": 8,\n",
        "    \"packaging defect\": 9,\n",
        "}\n",
        "\n",
        "df['label'] = df['hazard-category'].map(label2id)\n",
        "df['input'] = df['text']\n",
        "df = df[['input', 'label']]\n",
        "\n",
        "# Split into train and validation sets\n",
        "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert to Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "\n",
        "dataset = DatasetDict({'train': train_dataset, 'validation': val_dataset})\n",
        "\n",
        "# Define model checkpoint and label mappings\n",
        "model_checkpoint = 'openai-community/gpt2-large'\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_checkpoint, num_labels=10, id2label=id2label, label2id=label2id, device_map = 'auto'\n",
        ")\n",
        "print(model)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "# Add pad token if none exists\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Tokenize function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"input\"],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "# Apply tokenization\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Metrics\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels = p\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    accuracy_score = accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "    return {\"accuracy\": accuracy_score}\n",
        "\n",
        "# PEFT configuration\n",
        "peft_config = LoraConfig(\n",
        "    task_type=\"SEQ_CLS\",\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    target_modules=[\n",
        "        \"c_attn\",\n",
        "        \"c_proj\",\n",
        "        \"c_fc\",\n",
        "        \"mlp.c_proj\",\n",
        "    ]\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "print(model.print_trainable_parameters())\n",
        "\n",
        "# Training arguments\n",
        "lr = 2e-5\n",
        "batch_size = 8\n",
        "num_epochs = 5\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"gpt2-final-hazard\",\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    load_best_model_at_end=True\n",
        ")\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "model.save_pretrained(\"gpt2-final-hazard\")\n",
        "tokenizer.save_pretrained(\"gpt2-final-hazard\")"
      ],
      "metadata": {
        "id": "rs4tml4zs5FM"
      },
      "id": "rs4tml4zs5FM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b665b678-9e4d-4303-adc6-8bd35e4d842f",
      "metadata": {
        "id": "b665b678-9e4d-4303-adc6-8bd35e4d842f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "# Load the trained model and tokenizer\n",
        "model_path = \"Fine tuned model path\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels = 10)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Move model to the appropriate device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Label mapping\n",
        "id2label = {\n",
        "    0: \"allergens\",\n",
        "    1: \"biological\",\n",
        "    2: \"chemical\",\n",
        "    3: \"food additives and flavourings\",\n",
        "    4: \"foreign bodies\",\n",
        "    5: \"fraud\",\n",
        "    6: \"migration\",\n",
        "    7: \"organoleptic aspects\",\n",
        "    8: \"other hazard\",\n",
        "    9: \"packaging defect\"\n",
        "}\n",
        "\n",
        "# Prediction function\n",
        "def predict_hazard_category(model, tokenizer, input_text):\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(\n",
        "        input_text,\n",
        "        truncation=True,\n",
        "        max_length=256,\n",
        "        padding=\"max_length\",\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Move inputs to device\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    # Predict\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predicted_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    # Map to label\n",
        "    return id2label[predicted_class]\n",
        "\n",
        "df = pd.read_csv(\"TEST DATASET PATH\")\n",
        "\n",
        "\n",
        "# Apply predictions to the DataFrame\n",
        "df['predicted_hazard_category'] = df['text'].apply(\n",
        "    lambda text: predict_hazard_category(model, tokenizer, text)\n",
        ")\n",
        "\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0e4252a-1936-407e-9936-0a19afa6dc69",
      "metadata": {
        "id": "e0e4252a-1936-407e-9936-0a19afa6dc69"
      },
      "outputs": [],
      "source": [
        "#! ANOTHER WAY TO MAKE PREDICTIONS\n",
        "\n",
        "\n",
        "prediction_data = pd.read_csv(\"TEST DATASET PATH\")\n",
        "prediction_data['input'] = prediction_data['text']\n",
        "# Convert the data to a Hugging Face Dataset\n",
        "prediction_dataset = Dataset.from_pandas(prediction_data)\n",
        "\n",
        "# Tokenize the prediction data\n",
        "def tokenize_for_prediction(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"input\"],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "tokenized_prediction_dataset = prediction_dataset.map(tokenize_for_prediction, batched=True)\n",
        "\n",
        "raw_predictions = trainer.predict(tokenized_prediction_dataset)\n",
        "\n",
        "predictions = np.argmax(raw_predictions.predictions, axis=1)\n",
        "\n",
        "# Map numerical labels back to category names\n",
        "predicted_labels = [id2label[pred] for pred in predictions]\n",
        "\n",
        "# Add predictions to the DataFrame\n",
        "prediction_data[\"predicted-hazard-category\"] = predicted_labels\n",
        "# prediction_data.to_csv(\"predictions.csv\", index=False)\n",
        "\n",
        "print(prediction_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a56a617-a9f9-4661-b1ea-47f58083e950",
      "metadata": {
        "id": "2a56a617-a9f9-4661-b1ea-47f58083e950"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(prediction_data[\"hazard-category\"], prediction_data[\"predicted-hazard-category\"])\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}