{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cdab69b-40f5-43f3-9364-c5ef815caf39",
      "metadata": {
        "id": "0cdab69b-40f5-43f3-9364-c5ef815caf39"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# ! pip install evaluate\n",
        "# Load your dataset\n",
        "data = pd.read_csv('TRAIN DATASET PATH')\n",
        "\n",
        "# Prepare the data\n",
        "data['input'] = data.apply(lambda row: f\"\"\"Extract the exact reason for the food recall from the given text. Provide only the specific recall reason, without including any other information, from the following recall notice:\n",
        " Text: {row['text']}\"\"\", axis=1)\n",
        "data['target'] = data['hazard']\n",
        "\n",
        "# Save to a new file\n",
        "data[['input', 'target']].to_csv('prepared_dataset.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23a0f096-f2d3-4904-8639-46d19cafe741",
      "metadata": {
        "id": "23a0f096-f2d3-4904-8639-46d19cafe741"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "# HuggingFace and WandB login\n",
        "token = 'YOUR_HUGGING_FACE_TOKEN'\n",
        "login(token)\n",
        "os.environ[\"WANDB_API_KEY\"] = \"YOUR_WANDB_API_KEY\"\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset('csv', data_files='prepared_dataset.csv')\n",
        "\n",
        "# Split into training and validation sets\n",
        "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
        "train_dataset = dataset['train']\n",
        "val_dataset = dataset['test']\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"google/flan-t5-xl\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q\", \"k\", \"v\", \"EncDecAttention.q\", \"EncDecAttention.v\"]\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    # Ensure valid inputs and targets\n",
        "    inputs = tokenizer(examples['input'], max_length=512, truncation=True, padding=\"max_length\")\n",
        "    targets = tokenizer(examples['target'], max_length=32, truncation=True, padding=\"max_length\")\n",
        "    # Mask pad tokens in labels\n",
        "    inputs['labels'] = [\n",
        "        label if label != tokenizer.pad_token_id else -100\n",
        "        for label in targets['input_ids']\n",
        "    ]\n",
        "    return inputs\n",
        "\n",
        "\n",
        "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_val = val_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./flan-t5-peft-hazardfinal\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    fp16=False,\n",
        "    max_grad_norm=1.0,\n",
        "    push_to_hub=False,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Save the trained model and tokenizer\n",
        "model.save_pretrained(\"flan-t5-peft-hazardfinal\")\n",
        "tokenizer.save_pretrained(\"flan-t5-peft-hazardfinal\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fine_tuned_extractor = pipeline(\"text2text-generation\", model=\"Fine tuned model path\")\n",
        "\n",
        "\n",
        "def extract_food_product(text, title):\n",
        "    \"\"\"Uses the fine-tuned model to extract the food product.\"\"\"\n",
        "    text = title + \": \" + text\n",
        "    prompt = f\"Extract the exact reason for the food recall from the given text. Provide only the specific recall reason, without including any other information, from the following recall notice:{text}\" # Product extraction prompt\n",
        "    result = fine_tuned_extractor(prompt, max_length=32)[0]['generated_text']\n",
        "    # print(result)\n",
        "    return result.strip().lower()\n",
        "\n",
        "\n",
        "chunk_size = 500\n",
        "output_file = 'file1.csv'\n",
        "\n",
        "header = True\n",
        "i = 0\n",
        "for chunk in pd.read_csv('TEST DATASET PATH', chunksize=chunk_size):\n",
        "    chunk['extracted_hazard'] = chunk.apply(lambda row: extract_food_product(row['text'], row['title']), axis=1)\n",
        "    chunk.to_csv(output_file, mode='a', index=False, header=header)\n",
        "    header = False\n",
        "    i = i+1\n",
        "    print(f\"A chunk {i*50} over\\n\")\n",
        "\n",
        "print(\"Batch extraction complete. The updated dataset has been saved as 'updated_dataset_with_product.csv'.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "cnRwRgtvHP6q"
      },
      "id": "cnRwRgtvHP6q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "151d2ee7-46d6-45a0-a058-0b03c64467f2",
      "metadata": {
        "id": "151d2ee7-46d6-45a0-a058-0b03c64467f2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# ! pip install evaluate\n",
        "# Load your dataset\n",
        "data = pd.read_csv('TRAIN DATASET PATH')\n",
        "\n",
        "\n",
        "data['input'] = f\"Extract the exact food product from the given text without including any other information: {data['text']+data['title']}\"\n",
        "data['target'] = data['product']\n",
        "\n",
        "# Save to a new file\n",
        "data[['input', 'target']].to_csv('prepared_dataset2.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2896a4ac-134f-4d18-ad16-baf377d43620",
      "metadata": {
        "id": "2896a4ac-134f-4d18-ad16-baf377d43620"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "# HuggingFace and WandB login\n",
        "token = 'YOUR_HUGGING_FACE_TOKEN_HERE'\n",
        "login(token)\n",
        "os.environ[\"WANDB_API_KEY\"] = \"YOUR_WANDB_APT_KEY\"\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset('csv', data_files='prepared_dataset2.csv')\n",
        "\n",
        "# Split into training and validation sets\n",
        "dataset = dataset['train'].train_test_split(test_size=0.1)\n",
        "train_dataset = dataset['train']\n",
        "val_dataset = dataset['test']\n",
        "\n",
        "# Load tokenizer and model\n",
        "model_name = \"google/flan-t5-xl\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q\", \"k\", \"v\", \"EncDecAttention.q\", \"EncDecAttention.v\"]\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    # Ensure valid inputs and targets\n",
        "    inputs = tokenizer(examples['input'], max_length=512, truncation=True, padding=\"max_length\")\n",
        "    targets = tokenizer(examples['target'], max_length=32, truncation=True, padding=\"max_length\")\n",
        "    # Mask pad tokens in labels\n",
        "    inputs['labels'] = [\n",
        "        label if label != tokenizer.pad_token_id else -100\n",
        "        for label in targets['input_ids']\n",
        "    ]\n",
        "    return inputs\n",
        "\n",
        "# Apply preprocessing\n",
        "tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_val = val_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./flan-t5-peft-product_extract\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=50,\n",
        "    save_total_limit=2,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    fp16=False,\n",
        "    max_grad_norm=1.0,\n",
        "    push_to_hub=False,\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# Trainer setup\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Start training\n",
        "trainer.train()\n",
        "\n",
        "# Save the trained model and tokenizer\n",
        "model.save_pretrained(\"flan-t5-peft-product_extract\")\n",
        "tokenizer.save_pretrained(\"flan-t5-peft-product_extract\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c404116-4564-4d39-9828-86a7b1a035c0",
      "metadata": {
        "id": "1c404116-4564-4d39-9828-86a7b1a035c0"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "fine_tuned_extractor = pipeline(\"text2text-generation\", model=\"Fine tuned model path\")\n",
        "\n",
        "\n",
        "def extract_food_product(text, title):\n",
        "    \"\"\"Uses the fine-tuned model to extract the food product.\"\"\"\n",
        "    text = title + \": \" + text\n",
        "    prompt = f\"Extract the recall food product from the following food recall text: {text}\" # Product extraction prompt\n",
        "    result = fine_tuned_extractor(prompt, max_length=32)[0]['generated_text']\n",
        "    # print(result)\n",
        "    return result.strip().lower()\n",
        "\n",
        "\n",
        "chunk_size = 500\n",
        "output_file = 'file2.csv'\n",
        "\n",
        "header = True\n",
        "i = 0\n",
        "for chunk in pd.read_csv('TEST DATASET PATH', chunksize=chunk_size):\n",
        "    chunk['extracted_hazard'] = chunk.apply(lambda row: extract_food_product(row['text'], row['title']), axis=1)\n",
        "    chunk.to_csv(output_file, mode='a', index=False, header=header)\n",
        "    header = False\n",
        "    i = i+1\n",
        "    print(f\"A chunk {i*50} over\\n\")\n",
        "\n",
        "print(\"Batch extraction complete. The updated dataset has been saved as 'updated_dataset_with_product.csv'.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0751a1ad-8a43-4994-a735-5fa6b64754c9",
      "metadata": {
        "id": "0751a1ad-8a43-4994-a735-5fa6b64754c9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19881882-ba3e-4130-9534-ceb92104f692",
      "metadata": {
        "id": "19881882-ba3e-4130-9534-ceb92104f692"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "540be9fc-2231-48b4-a962-9b9f27402cf2",
      "metadata": {
        "id": "540be9fc-2231-48b4-a962-9b9f27402cf2"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}